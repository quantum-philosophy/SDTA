\subsection{Iterative HotSpot Simulation}
Before coming to the proposed solution, we first describe what can be done with the HotSpot simulator without extra efforts. A rough approximation of the SSDTP can be obtained by running the simulator with a very long power profile composed of one repeating chunk. In this case, HotSpot produces a temperature profile of the same enormous length, and we can take only the last part of it that corresponds to the last chunk of the power profile. Another way is to call the simulator a number of times each time starting from the temperature that we got during the previous call.

\image{hotspot-one-realization}{50 200 50 200}{An example of one HotSpot simulation for one processing element running an application of $1 s$. The ground truth is the real steady-state dynamic temperatures curve that HotSpot is supposed to reach after a number of iterations. The power profile contains $10^4$ steps that corresponds to $10^{-4} s$ sampling interval.}
The number of iterations required to get the steady-state temperature profile depends on the accuracy that we are trying to achieve. It also depends a lot on the application itself and, consequently, on its power profile. This is an especially urgent issue for tasks that have a short execution time relative to the thermal time constant of the die. In this case temperature does not have time to reach its steady-state value during executing of a task, but it is still gradually increasing on average. This leads to a large number iterations that the simulator is required to perform. An example is given on \figref{fig:hotspot-one-realization}, where the blue curve represents the ground truth (SSDTC), and the orange one shows one first simulation of HotSpot. The sampling interval was chosen to be $10^{-4} \; s$ to capture all power fluctuations due to the task switching activity, resulting in a power profile with $10^4$ steps. The situation is getting \emph{much worse} if we want to take into consideration the leakage power, since in this case we have a loop between temperature and power, and we need to perform extra iterations (indifferent of the chosen solution approach) for temperature and power to converge \cite{liu2007}.

\subsection{Analytical Solution}
In order to resolve the problem, we use an analytical approach. The direct solution of \equref{eq:initial} is given by the following equation:
\begin{equation} \label{eq:solution}
  T(t) = e^{C^{-1}A t} \; T_0 + (C^{-1} A)^{-1}(e^{C^{-1}A t} - I)C^{-1} B
\end{equation}

The solution provides us with the transient temperature and holds only when the power vector $B$ is constant. If it is not the case, we need to simulate shorter time intervals where this assumption can take place. Before going to the steady-state case, we perform one important adjustment to the system in order to be more efficient in our future calculations. According to \equref{eq:solution}, we need to compute the matrix exponential of the matrix $C^{-1} A t$. It would be much easier to accomplish if the matrix were symmetric, because a real symmetric matrix is \emph{diagonalizable} and has \emph{independent} (orthogonal) real eigenvectors:
\begin{equation} \label{eq:eigenvalue-decomposition}
  M = U \Lambda U^T
\end{equation}
where $M$ is a real symmetric matrix, $U$ is a square matrix of the eigenvectors of $M$, $\Lambda$ is a diagonal matrix composed of the eigenvalues of $M$ ($\lambda_i$), the equation itself is called the eigenvalue decomposition. Once we have such decomposition, the matrix exponential becomes a trivial task:
\begin{align}
  & e^M = e^{U \Lambda U^T} = U \: e^{\Lambda} \: U^T \nonumber \\
  & e^{\Lambda} = \left[
      \begin{array}{ccc}
        e^{\lambda_1} & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & e^{\lambda_{n}}
      \end{array}
    \right] \nonumber
\end{align}

Hence, instead of $C^{-1} A t$ in front of the variable vector we want to have a symmetry matrix. In order to achieve this, we perform the following substitution:
\begin{align*}
  Y & = C^{\frac{1}{2}} T \\
  D & = C^{-\frac{1}{2}} A \: C^{-\frac{1}{2}} \\
  E & = C^{-\frac{1}{2}} B
\end{align*}
with the result:
\begin{align}
  \frac{dY}{dt} & = D \: Y + E \nonumber \\
  Y(t) & = e^{D t} Y_0 + D^{-1} (e^{D t} - I) E \label{eq:modified-solution} \\
  T(t) & = C^{-\frac{1}{2}} Y(t) \label{eq:finalization}
\end{align}

In this case, $D$ is a symmetric matrix, therefore, it will be easier to find the matrix exponential of $D \: t$ using the above-mentioned eigenvalue decomposition (\equref{eq:eigenvalue-decomposition}):
\[
  e^{D t} = U \: e^{\Lambda t} \: U^T = U \left[
      \begin{array}{ccc}
        e^{t \lambda_1} & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & e^{t \lambda_{N_n}}
      \end{array}
    \right] U^T
\]

Now we shift our focus at the power profile $B$ and come closer to the SSDTP. Each row of $B$ corresponds to a particular time interval $\triangle t_i$ and represents the power consumption $B_i$ during this interval of all processing elements. Each step $i = 1 \dots N_s$ of the iterative process we have a pair $(\triangle t_i, B_i)$ which gives us a temperature vector $T_i$ according to \equref{eq:modified-solution} where $t = \triangle t_i$. The iterative process can be described as the following:
\begin{align}
  & Y_{i+1} = K_i \: Y_i + G_i \: B_i \label{eq:recurrent-equation} \\
  & K_i = e^{D \: \triangle t_i} \nonumber \\
  & G_i = D^{-1} \left( e^{D \triangle t_i} - I \right) C^{-\frac{1}{2}} \nonumber
\end{align}

Since we perform the eigenvalue decomposition of D (\equref{eq:eigenvalue-decomposition}), $D^{-1}$ can be efficiently computed in the following way:
\[
  D^{-1} = U \: \Lambda^{-1} \: U^T = U \left[
      \begin{array}{ccc}
        \frac{1}{\lambda_1} & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & \frac{1}{\lambda_{N_n}}
      \end{array}
    \right] U^T \\
\]
therefore:
\begin{align*}
  G_i & = U \: \Lambda^{-1} \: U^T \left(U \: e^{\Lambda \triangle t_i} \: U^T - U \: U^T \right) C^{-\frac{1}{2}} = \\
      & = U \left[
        \begin{array}{ccc}
          \frac{e^{\triangle t_i \: \lambda_1} - 1}{\lambda_1} & \cdots & 0 \\
          \vdots & \ddots & \vdots \\
          0 & \cdots & \frac{e^{\triangle t_i \: \lambda_{N_n}} - 1}{\lambda_{N_n}}
        \end{array}
      \right] U^T \: C^{-\frac{1}{2}}
\end{align*}

Therefore, in order to find SSDTC, we need to solve the following system of linear equations:
\[
  \begin{cases}
    K_1 \: Y_1 - Y_2 & = -Q_1 \\
    ... \\
    K_{N_s} \: Y_{N_s} - Y_{N_s + 1} & = -Q_{N_s}
  \end{cases}
\]
where $Q_i = G_i \: B_i$. Also we should take into account the boundary condition which ensures that the temperature has the same values on both sides of the curve:
\begin{equation} \label{eq:boundary-condition}
  Y_1 = Y_{N_s + 1}
\end{equation}

Hence, the system of linear equations takes the following form:
\[
  \begin{cases}
    K_1 \: Y_1 - Y_2 & = -Q_1 \\
    ... \\
    -Y_1 + K_{N_s} \: Y_{N_s} & = -Q_{N_s}
  \end{cases}
\]

To get the whole picture, the system can be written as:
\begin{align}
  & \mathbb{A} \: \mathbb{Y} = \mathbb{B} \label{eq:system} \\
  & \mathbb{A} = \left[
    \begin{array}{ccccc}
      K_1 & -I & 0 & \cdots & 0 \\
      0 & K_2 & -I &  & \vdots \\
      \vdots &  & \ddots & -I & 0 \\
      0 &  &  & K_{N_s - 1} & -I \\
      -I & 0 & \cdots & 0 & K_{N_s}
    \end{array}
  \right] \nonumber \\
  & \mathbb{Y} = \left[
    \begin{array}{c}
      Y_1 \\
      \vdots \\
      Y_{N_s}
    \end{array}
  \right] \nonumber \\
  & \mathbb{B} = \left[
    \begin{array}{c}
      -Q_1 \\
      \vdots \\
      -Q_{N_s}
    \end{array}
  \right] \nonumber
\end{align}

where $\mathbb{A}$ is a square matrix of the dimensions $N_n N_s \times N_n N_s$. $\mathbb{Y}$ and $\mathbb{B}$ are vectors of the length $N_n N_s$.

Apparently, we have obtained a regular system of liner equations with the SSDTP as its solution ($Y$ should also be processed with \equref{eq:finalization} in order to return back to $T$). The first straight-forward way to resolve the system is to use the LU factorization (decomposition). The problem here is that such systems could be extremely large, especially when we want to achieve a higher level of accuracy and, therefore, the power profile contains a lot of steps $N_s$. Each new step is $N_n$ new equations in the system given by \equref{eq:system}. Also the complexity grows very rapidly with the number of processing elements $N_p$, since in the HotSpot thermal model the number of thermal nodes $N_n$ dependents on it according to the equation \cite{rao2008}:
\[
  N_n = 4 N_p + 12
\]

Therefore, \emph{each} new processing element increases \emph{each} matrix $K_i$ by 4 rows and 4 columns, and \emph{each} vector $Y_i$ and $Q_i$ by 4 elements. As an example, if the power profile for a single-processor system is composed of 1000 steps, then having the same discretization but with one additional core results in a linear system with 4000 additional equations. All in all, a fast and accurate approach to solve \equref{eq:system} is required.

\image{sparseness-of-system}{50 200 50 200}{The sparseness of the system that we need to solve in order to obtain the SSDTP. Each blue point corresponds to a non-zero element of the matrix. All non-zero elements are located on the block diagonal of the matrix, one supdiagonal, and one subdiagonal in the left bottom corner.}
One may notice that the matrix $\mathbb{A}$ is an extremely sparse matrix with a very specific structure, it can be observed on \figref{fig:sparseness-of-system}. The matrix has non-zero elements only on its block diagonal (composed of matrices $N_n \times N_n$), one \emph{sup}diagonal just above the block diagonal, and one \emph{sub}diagonal in the left bottom corner of it. Therefore, instead of the dense LU decomposition we can apply algorithms that are specially designed for such cases. In our experiments we use the UMFPACK library, a set of routines for solving unsymmetric sparse linear systems based on the Unsymmetric MultiFrontal method \cite{umfpack2004}.

\subsection{Condensed Equation}
Now we shall propose a much faster solution. Let us return back to the system of linear equations that we are to solve. It is described with the following recurrence:
\begin{equation} \label{eq:ce-recurrent}
  Y_{i + 1} = K_i \: Y_i + Q_i, \; i = 1 \dots N_s
\end{equation}

Systems with similar structures can be found in multiple shooting methods for boundary value problems of ordinary differential equations \cite{stoer2002}. A common technique to solve such systems is to form a so-called \emph{condensed equation}, or \emph{condensed system}. Let us undertake this procedure step by step.

The iterative repetition of this equation leads us to:
\begin{equation} \label{eq:y-recurrent}
  Y_i = \prod_{j = 1}^{i} K_j \: Y_1 + P_{i - 1}, \; i = 2 \dots N_s + 1
\end{equation}
where $P_i$ are defined as the following:
\begin{align}
  P_1 & = Q_1 \nonumber \\
  P_i & = \sum_{l = 2}^i \prod_{j = l}^i K_j \: Q_{l - 1} + Q_i, \: i = 2 \dots N_s \nonumber \\
  P_i & = K_i \: P_{i - 1} + Q_i, \; i = 2 \dots N_s \label{eq:p-recurrent}
\end{align}

Therefore, we can calculate the final value $Y_{N_s + 1}$ from \equref{eq:y-recurrent}:
\[
  Y_{N_s + 1} = \prod_{j = 1}^{N_s} K_j \: Y_1 + P_{N_s}
\]

Taking into account the boundary condition given by \equref{eq:boundary-condition}, we obtain the following system of linear equations:
\[
  (I - \prod_{j = 1}^{N_s} K_j) \: Y_1 = P_{N_s}
\]

Solving this system, we find the first component of the vector $\mathbb{Y}$, that is $Y_1$. ($P_{N_s}$ can be calculated using \equref{eq:p-recurrent}.) All other vectors $Y_i$ for $i = 2 \dots N_s$ are successively found with help of \equref{eq:ce-recurrent}.

We also recall that $K_i$ is the matrix exponential, therefore, we use the following simplification:
\[
  \prod_{j = i}^l K_j = \prod_{j = i}^l e^{D t_j} = e^{D \sum_{j = i}^l t_j}
\]
since the product of each pair $D \: t_j$ and $D \: t_k$ is commutative.

The experimental results show that this approach has the highest performance without any loss of accuracy among other methods that we have considered in our analysis. Before going to the comparison, let us perform an important modification to the system given in the following subsection.

\subsection{Sampling Interval}
Now, we make an important assumption about the time intervals $\triangle t_i$ in order to perform all the calculations in a much more efficient manner. We assume that \emph{the time intervals are equal}, $\triangle t_i = \triangle t$ for $i = 1 \dots N_s$, i.e., the distance in time between two successive power measurements stays constant. We refer to this distance as the \emph{sampling interval}. The preferable size of this sampling interval depends on a particular application and the level of accuracy that we want to achieve. Having this assumption, the recurrent process (\equref{eq:recurrent-equation}) turns into:
\[
  Y_{i+1} = K \: Y_i + G \: B_i
\]
where:
\begin{align*}
  & K = e^{D \: \triangle t} \\
  & G = D^{-1} \left( e^{D \: \triangle t} - I \right) C^{-\frac{1}{2}}
\end{align*}

It should be noted that now $K$ and $G$ are constants, since they depend only on the matrices $D$, $C$, and the sampling interval $\triangle t$, which is fixed. In this case, the block diagonal of the matrix $\mathbb{A}$ in \equref{eq:system} is composed of the same repeating block $K_i = K$ for $i = 1 \dots N_s$. The matrix of the system becomes a block Toeplitz matrix, because inner blocks $\mathbb{A}_{ij}$ satisfy the following criterion:
\[
  \mathbb{A}_{ij} = \mathbb{A}_{i+1, j+1}, \; i, j = 1 \cdots N_s - 1
\]

To be more specific, the matrix is a block-circulant matrix where each block row vector is rotated one block element to the right relative to the preceding block row vector. This leads us to a wide range of possible techniques to solve \mbox{$\mathbb{A} \: \mathbb{Y} = \mathbb{B}$}, for example, the Fast Fourier Transform \cite{mazancourt1983}, \cite{vescovo1997}. In spite of the fact that this approach is \emph{much faster} then the solution obtained with the UMFPACK, our experiments have shown that the condensed equation method is even faster, therefore, we return back to it.

The recurrent expressions in case of equal time intervals are the following:
\begin{align}
  & Y_{i + 1} = K \: Y_i + Q_i, \; i = 1 \dots N_s \nonumber \\
  & P_i = K \: P_{i - 1} + Q_i, \; i = 2 \dots N_s \nonumber \\
  & (I - K^{N_s}) Y_1 = P_{N_s} \label{eq:linear-system}
\end{align}

Again thank to the matrix exponential $K$ and eigenvalue decomposition, $K^{N_s}$ can be found very efficiently:
\begin{align*}
  K^{N_s} & = U \: e^{N_s \triangle t \: \Lambda} \: U^T = U \: e^{\mathcal{T} \Lambda} \: U^T \\
    & = U \left[
      \begin{array}{ccc}
        e^{\mathcal{T} \lambda_1} & \cdots & 0 \\
        \vdots & \ddots & \vdots \\
        0 & \cdots & e^{\mathcal{T} \lambda_{N_n}}
      \end{array}
    \right] U^T
\end{align*}
where $U$ is a square matrix of the eigenvectors (orthogonal) of $D \triangle t$, $\Lambda$ is a diagonal matrix of the eigenvalues, and $\mathcal{T}$ is the period of the application.

Substituting $K^{N_s}$ from the last equation into \equref{eq:linear-system}, we get:
\[
  (I - U \: e^{\mathcal{T} \Lambda} \: U^T) Y_1 = P_{N_s}
\]

The identity matrix $I$ can be thought as $U U^T$, therefore:
\begin{align*}
  & U (I - e^{\mathcal{T} \Lambda}) U^T \: Y_1 = P_{N_s} \\
  & Y_1 = U (I - e^{\mathcal{T} \Lambda})^{-1} U^T P_{N_s} \\
  & Y_1 = U M U^T P_{N_s}
\end{align*}
where $M$ is a diagonal matrix with the following structure:
\[
  M = \left[
    \begin{array}{ccc}
      \frac{1}{1 - e^{\mathcal{T} \lambda_1}} & \cdots & 0 \\
      \vdots & \ddots & \vdots \\
      0 & \cdots & \frac{1}{1 - e^{\mathcal{T} \lambda_{N_n}}}
    \end{array}
  \right]
\]

As we see, in this approach there is no need to inverse any matrix, the solution of the system is obtained by scalar divisions and a similarity transformation with $U$.

\subsection{Performance Comparison}
\image{application-time}{50 200 50 200}{A performance comparison on the semilogarithmic scale of three methods for the SSDTP calculation: the HotSpot simulator (HS), the Unsymmetric MultiFrontal method (UMF), and the Condensed Equation method (CE). The performance is given for applications with different periods. The sampling interval is constant and equal to 1 millisecond, 1 second on the horizontal axis corresponds to 1000 steps in the power profile.}
So far we have mentioned several ways to perform the steady-state dynamic temperature profile calculation:
\begin{itemize}
  \item The iterative solution with the HotSpot simulator.
  \item Direct dense solutions, namely, the dense LU decomposition (the Gaussian elimination).
  \item Direct sparse solutions with such techniques as the Unsymmetric MultiFrontal method.
  \item Solutions for Toeplitz matrices, and in particular for block-circulant matrices, e.g., the Fast Fourier Transform.
  \item The solution based on the Condensed Equation method.
\end{itemize}

In should be noted that the first solution, the HotSpot simulator, has nothing to do with the system of linear equations given by \equref{eq:system}, which is not the case with the other approaches. The second solution, the LU decomposition, is not a competitor to the rest of the methods, since it does not take into account any special properties of the system. Moreover, direct solvers (in this case, the LU decomposition and the Unsymmetric MultiFrontal method) are known to consume a lot of memory and, therefore, have problems with considerably large systems of linear equations. The last two seem to be the most promising solutions, although, according to our experiments, the Condensed Equation method is significantly faster than the FFT. The explanation to this is the following. The approach based on the fact that the matrix of the system is a block-circulant matrix is aware of the recurrent nature of the system, but it does not consider its sparseness, while the Condensed Equation has both of these features.

Another possible technique, that we considered, but do not discuss in details due to the shortage of space, is iterative methods for solving systems of linear systems (e.g. the Jacobi, Gauss–Seidel, Successive over-relaxation methods). These methods are designed to overcome problems of direct solvers. For instance, they do not operate on full matrices and, therefore, they consume much less memory and can be applied for large systems. The most important issues with these methods are their convergence and accuracy. In our analysis we did not observe any advantages of using this methods for this particular problem, they demonstrated slow convergence and low accuracy.

Now we shall compare three of the methods listed above: the HotSpot simulator, the Unsymmetric MultiFrontal method, and the Condensed Equation method. The comparison is given on \figref{fig:application-time} where we vary the period of the simulated application keeping a constant sampling interval of 1 millisecond. It is basically proportionally corresponds to the number of steps in the power profile (1 second of the application time equals to 1000 steps in the power profile).
