\isubsection{SSDTP Calculation} \label{sec:results-ssdtp}
\input{include/table-optimization.tex}
In this subsection we investigate the scaling properties of the proposed solution for the SSDTP calculation and compare it with the approach based on the TTA with HotSpot (\secref{sec:hotspot-iterative-solution})\footnote{All the experiments are performed on a Linux machine with Intel\textregistered\ Core\texttrademark\ i7-2600 3.4GHz and 8Gb of RAM.}. We also include in the comparison two additional techniques described in the appendix, namely the TTA with the analytical solution (\appref{tta-analytical}) and the fast Fourier transform (FFT) (\appref{straight-forward}). In the cases of the TTA, the simulation over successive iterations is run until the NRMSE relative to the SSDTP obtained with the proposed method is less than 1\%.

In the following experiments, the power sampling interval is set to 1 $ms$ and the thermal configuration of the die is the same as in \tabref{tab:parameters}. For the experiments in this subsection, the leakage power has not been considered. If considered according to the linearized model (\secref{sec:linearized-leakage}), execution times remain unchanged; if considered according to the iterative model (\secref{sec:iterative-leakage}), execution times increase proportionally for all the methods, which does not affect any of the conclusions.

First, we vary the application period $\period$ keeping the architecture fixed, which is a quad-core platform with the core area of 4 $mm^2$. The comparison is depicted in \figref{fig:scaling-time} on a semilogarithmic scale. It can be seen that the proposed technique is roughly 5000 times faster than calculating the SSDTP by running the TTA with HotSpot and from 9 to 170 times faster than the TTA with the analytical solution.

In the second experiment we evaluate the scaling of the proposed method with regard to the number of processing elements. The application period is fixed to 0.5 $s$. The results are shown in \figref{fig:scaling-cores}. It can be observed that the proposed technique provides a significant performance improvement relative to the alternative solutions.

\isubsection{Reliability Optimization} \label{sec:reliability-results}
In this section we evaluate the reliability optimization approach described in \secref{sec:reliability}, first with a set of synthetic applications and, finally, using a real-life example.

The experimental setup is the following. Heterogeneous platforms and periodic applications are generated randomly \cite{dick1998} in such a way that the execution time of tasks is uniformly distributed between 1 and 10 $ms$ and the leakage power accounts for 30--60\% of the total power dissipation\footnote{The parameters of the applications and platforms (task graphs, floorplans, HotSpot configurations, etc.) used in our experiments are available online at \cite{liu2011}.}. The linear leakage model is used in the experiments, since, as discussed in \secref{sec:linearized-leakage}, it provides a good approximation. The area of one core is 4 $mm^2$, other parameters of the die and thermal package are given in \tabref{tab:parameters}. The temperature constraint $T_{max}$ (see \equref{eq:t-max}) is set to $100^\circ C$. In \equref{eq:cycles-to-failure} the Coffin-Manson exponent $b$ is set to 6, the activation energy $E_a$ to 0.5, and the elastic temperature region $\Delta T_0$ to zero \cite{jedec2010}. The coefficient of proportionality $A$ is not significant, since we are concerned about the relative improvement.

In each of the experiments, we compare the optimized solution with an initial temperature-aware solution proposed in \cite{xie2006}. This solution consists of a task mapping and schedule that captures the spatial temperature behavior and tries to minimize the peak temperature while satisfying the real-time constraints. The deadline is set to the duration of the initial schedule extended by 5\%.

In the first set of experiments, we change the number of cores $N_p$ while keeping the number of tasks $N_t$ per core constant and equal to 20. For each problem we have generated 20 random task graphs and found the average improvement of the MTTF over the initial solution ($\mttfimp$). We also have measured the change in the consumed energy ($\eimp$). The results are given in \tabref{tab:mttf-cores} ($t$ indicates the optimization time in seconds). It can be seen that the reliability-aware optimization dramatically increases the MTTF by 13 up to 40 times. Even for large applications with, e.g., 320 tasks deployed onto 16 cores, a feasible mapping and schedule that significantly improve the lifetime of the system can be found in an affordable time. Moreover, our optimization does not impact the energy efficiency of the system.

For the second set of experiments, we keep the quad-core architecture and vary the size (number of tasks $N_t$) of the application. The number of randomly generated task graphs per application size is 20. The average improvement of the MTTF along with the change in the energy consumption are given in \tabref{tab:mttf-tasks}. The observations are similar to those for the previous set of experiments.

The above experiments have confirmed that our proposed approach is able to effectively increase the MTTF of the system. The efficiency of this approach is due to the fast and accurate SSDTP calculation, which is at the heart of the optimization, and which, due to its speed, allows a huge portion of the design space to be explored. In order to prove this, we have replaced, inside our optimization framework, the proposed SSDTP calculation with the calculation based on HotSpot (\secref{sec:hotspot-iterative-solution}) and based on the SSA (\secref{sec:steady-state-approximation}), respectively. The goal is to compare our results with the results produced using HotSpot and the SSA, after the same optimization time as needed with the proposed SSDTP calculation technique. The experimental setup is the same as for the experiments in \tabref{tab:mttf-tasks}. The MTTF obtained with HotSpot and the SSA is evaluated and compared with the MTTF obtained by our proposed method. The results are summarized in \tabref{tab:mttf-comparison}. For example, the lifetime of the platform running 160 tasks can be extended by more than 18 times, compared to the initial solution, using our approach, whereas, the best solutions found with HotSpot and the SSA, using the same optimization time, are only 2.02 and 5.33 times better, respectively. The reason for the poor results with HotSpot is the excessively long execution time of the SSDTP calculation. This allows for a much less thorough investigation of the solution space than with our proposed technique. In the case of the SSA, the reason is different. The SSA is fast but also very inaccurate (\secref{sec:steady-state-approximation}). The inaccuracy drives the optimization towards solutions that turn out to be of low quality.

We have seen that our reliability-targeted optimizations have significantly increased the MTTF without affecting the energy consumption. This is not surprising, since our optimization will search towards low temperature solutions, which implicitly means low leakage. In order to further explore this aspect, we have performed a multi-objective optimization\footnote{The multi-objective optimization is based on NSGA-II \cite{deb2002}.} along the dimensions of energy and reliability. An example of the Pareto front averaged over 20 applications with 80 tasks deployed onto a quad-core platform is given in \figref{fig:average-pareto}. It can be observed that the variation of energy is less than 2\%. This means that solutions optimized for the MTTF have an energy consumption almost identical to those optimized for energy. At the same time, the difference along the MTTF is huge. This means that ignoring the reliability aspect one may end up with a significantly decreased MTTF, without any significant gain in energy.

Finally, we have applied our optimization technique to a real-life example, namely the MPEG2 video decoder \cite{ffmpeg2011} that is deployed onto a dual-core platform. The decoder was analyzed and split into 34 tasks. The parameters of each task were obtained through a system-level simulation using MPARM \cite{benini2005}. The deadline is set to 40 $ms$ assuming 25 video frames per second. The solution found with the proposed method improves the lifetime of the system by 23.59 times with a 5\% energy saving, compared to the initial solution. The same optimization was solved using HotSpot and the SSA. The best found solutions are only 5.37 and 11.50 times better than the initial one, respectively.
