\subsection{SSDTP Computation Performance} \label{sec:results-ssdtp}
In this subsection we investigate the scalability properties of the proposed solution based on the condensed equation method and compare it with one transient temperature simulation (TTS) of the application period, which is not sufficient to reach the SSDTP as it was shown in \secref{sec:hotspot-solution}. For the transient temperature analysis (TTA), the HotSpot thermal simulator is used.

\image{scaling-time}{80 230 80 230}{Scalability with the application period for a quad-core architecture. The sampling interval is fixed to 1 millisecond where 1 second on the horizontal axis corresponds to 1000 steps in the power profile. The comparison is given on the semilogarithmic scale.}
\begin{itable}{scaling-time}{|r|r|r|r|r|}
  {Scalability with the application period shown in \figref{fig:scaling-time}.}
  {$\mathcal{T}$ --- the application period, CE --- the Condensed Equation method, TTS --- one Transient Temperature Simulation, NRMSE --- the Normalized Root Mean Square Error.}
  \hline
  $\mathcal{T}$, s & CE, ms & TTS, ms & Speedup, $\times$ & NRMSE, \% \\
  \hline
  \hline
  0.05 &  0.18 &   10.24 & 56.93 & 25.8 \\
   0.1 &  0.35 &   20.26 & 58.30 & 19.0 \\
   0.5 &  1.63 &   97.36 & 59.73 & 9.65 \\
     1 &  3.23 &  193.31 & 59.80 & 7.80 \\
     2 &  6.48 &  382.59 & 59.08 & 6.46 \\
     3 &  9.58 &  573.15 & 59.83 & 5.79 \\
     4 & 12.78 &  770.09 & 60.25 & 5.34 \\
     5 & 16.10 &  964.75 & 59.92 & 5.00 \\
     6 & 19.32 & 1146.87 & 59.36 & 4.72 \\
     7 & 22.51 & 1335.26 & 59.31 & 4.49 \\
     8 & 25.69 & 1536.91 & 59.82 & 4.28 \\
     9 & 28.94 & 1729.39 & 59.76 & 4.09 \\
    10 & 32.65 & 1921.14 & 58.83 & 3.93 \\
  \hline
\end{itable}
First, we vary the application period keeping the sampling interval constant and equal to 1 millisecond. The comparison for a quad-core architecture is given in \figref{fig:scaling-time} and \tabref{tab:scaling-time}. It can be seen that the CE method is roughly 60 times faster than one iteration of the TTA\footnote{All the experiments are done on a Linux machine with Intel\textregistered\ Core\texttrademark\ i7-2600 (3.4GHz, 4 cores, 8 threads) and 8Gb of RAM.}. The application period proportionally corresponds to the number of steps in the power profile (one second is equal to 1000 steps in the power profile in the above-mentioned example). Hence, we would see the same curves, if we were investigating the dependency on the power profile discretization.

\image{scaling-cores}{80 230 80 230}{Scalability with the number of cores. The application period is fixed to 1 second that corresponds to 1000 steps in the power profile. The comparison is given on the semilogarithmic scale.}
\begin{itable}{scaling-cores}{|r|r|r|r|r|}
  {Scalability with the number of cores shown in \figref{fig:scaling-cores}.}
  {$N_p$ --- the number of processing elements (cores), CE --- the Condensed Equation method, TTS --- one Transient Temperature Simulation, NRMSE --- the Normalized Root Mean Square Error.}
  \hline
  $N_p$ & CE, ms & TTS, ms & Speedup, $\times$ & NRMSE, \% \\
  \hline
  \hline
    1 &    0.99 &    97.00 & 97.93 & 25.3 \\
   10 &   16.46 &   632.50 & 38.44 & 40.6 \\
   20 &   46.00 &  1761.75 & 38.30 & 69.5 \\
   30 &   98.49 &  3472.21 & 35.25 & 10.2 \\
   40 &  172.69 &  5827.77 & 33.75 & 130  \\
   50 &  266.08 &  8771.93 & 32.97 & 142  \\
   60 &  380.95 & 12235.91 & 32.12 & 185  \\
   70 &  517.04 & 16363.54 & 31.65 & 220  \\
   80 &  675.21 & 21104.98 & 31.26 & 245  \\
   90 &  856.20 & 26415.77 & 30.85 & 277  \\
  100 & 1058.35 & 32329.09 & 30.55 & 308  \\
  \hline
\end{itable}
The second part of the comparison is the scalability with the number of processing elements shown in \figref{fig:scaling-cores} and \tabref{tab:scaling-cores}. It can be seen that the difference between computation times of the CE method and one TTS becomes smaller when the number of cores is increasing. At the same time the mismatch between the SSDTP and temperature profile produced by one TTA dramatically increases, which means that larger number of the TTA iterations is required to reach the same level of accuracy.

\subsection{Reliability Optimization}
The optimization procedure is held by a genetic algorithm (GA) \cite{schmitz2004} that varies mapping and scheduling of the application in order to find the longest lifetime of the system. Each chromosome within the GA is a vector of $2 \times N_t$ elements, where $N_t$ is the number of tasks in the application. The first half of such a vector encodes priorities of the tasks, while the second encodes their mapping. To calculate schedules of all processing elements and, hence, the dynamic power profile of the whole system, a form of the list scheduler (LS) is used. Given a power profile, the corresponding SSDTP is computed by the CE method, which is further evaluated in terms of the reliability model given in \secref{sec:reliability}. The initial population is initialized partially randomly and partially based on the mobility of the tasks \cite{schmitz2004}. The size of the population is equal to $4 \times N_t$. The stopping condition is a stall during a certain number of successive generations. In order to count thermal cycles in temperature profiles, the rainflow counting method is employed \cite{xiang2010}. For the leakage modeling the linear approximation given in \secref{sec:leakage} is used.

In each of the experiments, we compare the optimized solution with the initial one that is obtained in the following way. First, we calculate the average execution time of each of the tasks, since it can vary from one core to another. Then we compute the mobility of the tasks and schedule the application according to it \cite{schmitz2004}. The mapping part is done along with the scheduling where each task is assigned to the earliest available core in the system.

\begin{itable}{mttf-cores}{|r|r|r|r|}
  {Reliability optimization for different architectures}
  {$N_p$ --- the number of cores, $N_t$ --- the number of tasks, Time --- the computational time, MTTF --- the average improvement of the MTTF.}
  \hline
  $N_p$ & $N_t$ & Time, m & MTTF, \% \\
  \hline
    2 &   20 & 0 & 0 \\
    4 &   40 & 0 & 0 \\
    8 &   80 & 0 & 0 \\
   16 &  160 & 0 & 0 \\
   32 &  320 & 0 & 0 \\
  \hline
\end{itable}
In the first set of experiments, we change the number of cores while keeping the number of tasks per core constant and equal to 10. For each problem we generate 20 random task graphs \cite{dick1998}, apply the optimization procedure, and state the average improvement relative to the initial solution. The results are given in \tabref{tab:mttf-cores}.

\begin{itable}{mttf-tasks}{|r|r|r|r|}
  {Reliability optimization for different applications}
  {$N_p$ --- the number of cores, $N_t$ --- the number of tasks, Time --- the computational time, MTTF --- the average improvement of the MTTF.}
  \hline
  $N_p$ & $N_t$ & Time, m & MTTF, \% \\
  \hline
  4 &  20 & 0 & 0 \\
  4 &  40 & 0 & 0 \\
  4 &  80 & 0 & 0 \\
  4 & 160 & 0 & 0 \\
  4 & 320 & 0 & 0 \\
  \hline
\end{itable}
For the second set of experiments, we keep a quad-core architecture and vary the number of tasks within the application. For each problem 20 task graphs are generated and the average improvement of the MTTF is given in \tabref{tab:mttf-tasks}.
