\subsection{SSDTP Computation Performance} \label{sec:results-ssdtp}
In this subsection we investigate the scalability properties of the proposed solution based on the CE method and compare it with one transient temperature simulation (TTS) of the application period, which is not sufficient to reach the SSDTP as it was shown in \secref{sec:hotspot-solution}. To perform the TTA, the HotSpot thermal simulator is used.

\image{scaling-time}{80 230 80 230}{Scalability with the application period for a quad-core architecture. The sampling interval is fixed to 1 millisecond where 1 second on the horizontal axis corresponds to 1000 steps in the power profile. The comparison is given on the semilogarithmic scale.}
\begin{itable}{scaling-time}{|r|r|r|r|r|}
  {Scalability with the application period shown in \figref{fig:scaling-time}.}
  {$\mathcal{T}$ --- the application period, CE --- the Condensed Equation method, TTS --- one Transient Temperature Simulation, NRMSE --- the Normalized Root Mean Square Error.}
  \hline
  $\mathcal{T}$, s & CE, ms & TTS, ms & Speedup, $\times$ & NRMSE, \% \\
  \hline
  \hline
  0.05 &  0.18 &   10.24 & 56.93 & 25.8 \\
   0.1 &  0.35 &   20.26 & 58.30 & 19.0 \\
   0.5 &  1.63 &   97.36 & 59.73 & 9.65 \\
     1 &  3.23 &  193.31 & 59.80 & 7.80 \\
     2 &  6.48 &  382.59 & 59.08 & 6.46 \\
     3 &  9.58 &  573.15 & 59.83 & 5.79 \\
     4 & 12.78 &  770.09 & 60.25 & 5.34 \\
     5 & 16.10 &  964.75 & 59.92 & 5.00 \\
     6 & 19.32 & 1146.87 & 59.36 & 4.72 \\
     7 & 22.51 & 1335.26 & 59.31 & 4.49 \\
     8 & 25.69 & 1536.91 & 59.82 & 4.28 \\
     9 & 28.94 & 1729.39 & 59.76 & 4.09 \\
    10 & 32.65 & 1921.14 & 58.83 & 3.93 \\
  \hline
\end{itable}
First, we vary the application period keeping the sampling interval constant and equal to 1 millisecond. The comparison for a quad-core architecture is given in \figref{fig:scaling-time} and \tabref{tab:scaling-time}. It can be seen that the CE method is roughly 60 times faster than one iteration of the TTA\footnote{All the experiments are done on a Linux machine with Intel\textregistered\ Core\texttrademark\ i7-2600 (3.4GHz, 4 cores, 8 threads) and 8Gb of RAM.}. The application period proportionally corresponds to the number of steps in the power profile (one second is equal to 1000 steps in the power profile in the above-mentioned example). Hence, we would see the same curves, if we were investigating the dependency on the power profile discretization.

\image{scaling-cores}{80 230 80 230}{Scalability with the number of cores. The application period is fixed to 1 second that corresponds to 1000 steps in the power profile. The comparison is given on the semilogarithmic scale.}
\begin{itable}{scaling-cores}{|r|r|r|r|r|}
  {Scalability with the number of cores shown in \figref{fig:scaling-cores}.}
  {$N_p$ --- the number of processing elements (cores), CE --- the Condensed Equation method, TTS --- one Transient Temperature Simulation, NRMSE --- the Normalized Root Mean Square Error.}
  \hline
  $N_p$ & CE, ms & TTS, ms & Speedup, $\times$ & NRMSE, \% \\
  \hline
  \hline
    1 &    0.99 &    97.00 & 97.93 &  25 \\
   10 &   16.46 &   632.50 & 38.44 &  41 \\
   20 &   46.00 &  1761.75 & 38.30 &  69 \\
   30 &   98.49 &  3472.21 & 35.25 & 102 \\
   40 &  172.69 &  5827.77 & 33.75 & 130 \\
   50 &  266.08 &  8771.93 & 32.97 & 142 \\
   60 &  380.95 & 12235.91 & 32.12 & 185 \\
   70 &  517.04 & 16363.54 & 31.65 & 220 \\
   80 &  675.21 & 21104.98 & 31.26 & 245 \\
   90 &  856.20 & 26415.77 & 30.85 & 277 \\
  100 & 1058.35 & 32329.09 & 30.55 & 308 \\
  \hline
\end{itable}
The second part of the comparison is the scalability with the number of processing elements shown in \figref{fig:scaling-cores} and \tabref{tab:scaling-cores}. It can be seen that the difference between computation times of the CE method and one TTS becomes smaller when the number of cores is increasing. At the same time the mismatch between the SSDTP and temperature profile produced by one TTA dramatically increases, which means that larger number of the TTA iterations is required to reach the same level of accuracy.

\subsection{Reliability Optimization}
The experimental setup is the following. Heterogeneous architectures and periodic applications are generated randomly \cite{dick1998} in such a way that the execution time of tasks is uniformly distributed between 1 and 50 milliseconds and the leakage power accounts for 30--55\% of the total power dissipation. The corresponding temperature variation lies between the ambient temperature of $27^{\circ}C$ and maximal temperature of $100^{\circ}C$.

In each of the experiments, we compare the optimized solution with the initial one that is obtained in the following way. First, we calculate the average execution time of each task among the processing elements. Then, we compute the mobility of the tasks and schedule the application according to it \cite{schmitz2004}. The mapping is done along with the scheduling where each task being considered is assigned to the earliest available core in the system. This combination of mapping and scheduling is the starting point for the future optimization. The deadline of the application is set according to the duration of the initial schedule with additional 5\%.

\begin{itable}{mttf-cores}{|r|r|r|r|r|}
  {Reliability optimization for different architectures}
  {$N_p$ --- the number of cores, $N_t$ --- the number of tasks, $t_{avg}$ --- the average computational time, $MTTF_{avg}$ --- the average MTTF improvement, $MTTF_{max}$ --- the maximal MTTF improvement.}
  \hline
  $N_p$ & $N_t$ & $t_{avg}, m$ & $MTTF_{avg}$, \% & $MTTF_{max}$, \% \\
  \hline
   2 &   20 &   0.16 & 129 & 324 \\
   4 &   40 &   1.10 & 185 & 369 \\
   8 &   80 &   7.76 & 159 & 250 \\
  16 &  160 &  34.59 & 191 & 269 \\
  32 &  320 & 101.62 & 191 & 452 \\
  \hline
\end{itable}
In the first set of experiments, we change the number of cores while keeping the number of tasks per core constant and equal to 10. For each problem we generate 20 random task graphs and find the average improvement. The results are given in \tabref{tab:mttf-cores}.

\begin{itable}{mttf-tasks}{|r|r|r|r|r|}
  {Reliability optimization for different applications}
  {$N_p$ --- the number of cores, $N_t$ --- the number of tasks, $t_{avg}$ --- the average computational time, $MTTF_{avg}$ --- the average MTTF improvement, $MTTF_{max}$ --- the maximal MTTF improvement.}
  \hline
  $N_p$ & $N_t$ & $t_{avg}, m$ & $MTTF_{avg}$, \% & $MTTF_{max}$, \% \\
  \hline
  4 &  20 & 0 & 0 & 0 \\
  4 &  40 & 0 & 0 & 0 \\
  4 &  80 & 0 & 0 & 0 \\
  4 & 160 & 0 & 0 & 0 \\
  4 & 320 & 0 & 0 & 0 \\
  \hline
\end{itable}
For the second set of experiments, we keep the quad-core architecture and vary the number of tasks within the application. The number of random task graphs per problem is 20. The average improvement of the MTTF is given in \tabref{tab:mttf-tasks}.
