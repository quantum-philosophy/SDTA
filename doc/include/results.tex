\subsection{Computation Performance} \label{sec:results-ssdtp}
In this subsection we investigate the scalability properties of the proposed solution for the SSDTP calculation and compare it with the approaches based on the FFT (\secref{sec:fast-fourier-transform}), TTA with the analytical solution (\secref{sec:tta-analytical}), and TTA with HotSpot (\secref{sec:hotspot-solution})\footnote{All the experiments are done on a Linux machine with Intel\textregistered\ Core\texttrademark\ i7-2600 (3.4GHz, 4 cores, 8 threads) and 8Gb of RAM.}. In the last two case, the TTA is run until the normalized RMSE relative to the SSDTP obtained with the CE method is less than 1\%. The sampling interval is equal to \mbox{1 $ms$}.

\image{scaling-time}{40 230 40 230}{Scalability with the application period for a quad-core architecture. The comparison is given on the semilogarithmic scale.}
\begin{itable}{scaling-time}{|r|r|r|r|r|r|r|}
  {Scalability with the application period shown in \figref{fig:scaling-time}.}
  {$\period$ --- the application period, CE --- the CE method in $ms$, FFT --- the speed-up relative to the FFT method, $TTA^{AS}$, $TTA^{HS}$ --- the speed-up relative to the TTA with the analytical solution and TTA with HotSpot, respectively, along with the number of iterations.}
  \hline
  \multirow{2}{*}{$\period$, s} & \multirow{2}{*}{CE, ms} & \multirow{2}{*}{FFT, $\times$} & \multicolumn{2}{c|}{$TTA^{AS}$} & \multicolumn{2}{c|}{$TTA^{HS}$} \\ \cline{4-7}
  & & & $\times$ & Periods & $\times$ & Periods \\
  \hline
  \hline
  0.05 & 0.18 & 52.12 & 168.71 & 385 & 6750.96 & 689 \\
  0.10 & 0.39 & 41.64 &  77.91 & 191 & 5301.97 & 268 \\
  0.20 & 0.67 & 41.64 &  42.79 &  95 & 5188.80 & 122 \\
  0.30 & 1.03 & 39.09 &  27.97 &  63 & 5030.25 &  86 \\
  0.40 & 1.36 & 39.16 &  21.43 &  48 & 4887.90 &  63 \\
  0.50 & 1.70 & 43.56 &  16.99 &  38 & 4880.57 &  50 \\
  0.60 & 2.04 & 40.86 &  14.34 &  32 & 4899.20 &  42 \\
  0.70 & 2.32 & 42.46 &  12.87 &  28 & 4935.89 &  36 \\
  0.80 & 2.66 & 41.70 &  11.07 &  24 & 4842.62 &  31 \\
  0.90 & 2.98 & 42.54 &  10.21 &  22 & 4883.76 &  28 \\
  1.00 & 3.33 & 45.19 &   9.23 &  20 & 4892.86 &  25 \\
  \hline
\end{itable}
First, we vary the application period keeping the architecture fixed, which is a quad-core platform with the core area of 4 $mm^2$ and configuration shown in \tabref{tab:parameters}. The comparison is depicted in \figref{fig:scaling-time}. The computational time of the CE method and its speed-up relative to the rest are given in \tabref{tab:scaling-time}. It can be seen that the proposed technique is roughly 5000 times faster than the TTA with HotSpot and from 9 to 170 times faster than the TTA with the analytical solution. The number of application periods, over which the TTA is performed, is significantly larger for short application periods (\secref{sec:hotspot-iterative-solution}) while longer periods imply larger numbers of steps in the power profiles keeping the cost of the analysis high.

\image{scaling-cores}{40 230 40 230}{Scalability with the number of cores. The comparison is given on the semilogarithmic scale.}
\begin{itable}{scaling-cores}{|r|r|r|r|r|r|r|}
  {Scalability with the number of cores shown in \figref{fig:scaling-cores}.}
  {$N_p$ --- the number of cores, CE --- the CE method in $ms$, FFT --- the speed-up relative to the FFT method, $TTA^{AS}$, $TTA^{HS}$ --- the speed-up relative to the TTA with the analytical solution and TTA with HotSpot, respectively, along with the number of iterations.}
  \hline
  \multirow{2}{*}{$N_p$} & \multirow{2}{*}{CE, ms} & \multirow{2}{*}{FFT, $\times$} & \multicolumn{2}{c|}{$TTA^{AS}$} & \multicolumn{2}{c|}{$TTA^{HS}$} \\ \cline{4-7}
  & & & $\times$ & Periods & $\times$ & Periods \\
  \hline
  \hline
   2 &  0.77 & 70.79 & 15.74 & 33 & 3236.75 & 45 \\
   4 &  1.63 & 46.09 & 17.68 & 38 & 2906.30 & 50 \\
   8 &  4.67 & 30.08 & 20.40 & 44 & 2695.69 & 55 \\
  16 & 15.48 & 21.08 & 24.36 & 54 & 2434.74 & 62 \\
  32 & 56.44 & 13.13 & 27.24 & 62 & 2214.19 & 65 \\
  \hline
\end{itable}
The second part of the comparison is the scalability with the number of processing elements shown in \figref{fig:scaling-cores} and \tabref{tab:scaling-cores}. The configuration of the chip is similar to the previous experiment. In can be observed that the proposed solution provides a significant performance improvement relative to its competitors where in order to keep the same level of accuracy the TTA requires larger number of periods of the application to be analyzed.

\subsection{Reliability Optimization} \label{sec:reliability-results}
In this section we present the results of the reliability optimization described in \secref{sec:reliability-problem}. The experimental setup is the following. Heterogeneous platforms and periodic applications are generated randomly \cite{dick1998} in such a way that the execution time of tasks is uniformly distributed between 1 and 20 $ms$ and the leakage power accounts for 30--60\% of the total power dissipation. The area of one core is 4 $mm^2$, other parameters of the die and thermal package are given in \tabref{tab:parameters}. The temperature constrain $T_{max}$ (see \equref{eq:t-max}) is set to $100^\circ C$. In \equref{eq:cycles-to-failure} the Coffin-Manson exponent $b$ is set to 6 and the activation energy $E_a$ to 0.5, and the elastic temperature region $\Delta T_0$ to zero. The coefficient of proportionality $A$ is indifferent, since we are concerned about the relative improvement.

In each of the experiments, we compare the optimized solution with the initial one that is obtained in the following way. First, we calculate the average execution time of each task among the processing elements. Then, we compute the mobility of the tasks and schedule the application according to it \cite{schmitz2004}. The mapping is done along with the scheduling with an approach similar to \cite{xie2006}. This combination of mapping and scheduling is the starting point for the future optimization. The deadline of the application is set according to the duration of the initial schedule with additional 5\%.

\begin{itable}{mttf-cores}{|r|r|r|r|r|}
  {Reliability optimization for different architectures}
  {$N_p$ --- the number of cores, $N_t$ --- the number of tasks, $t_{avg}$ --- the average computational time, $MTTF_{avg}$ --- the average MTTF improvement, $E_{avg}$ --- the average change in the energy consumption.}
  \hline
  $N_p$ & $N_t$ & $t_{avg}, m$ & $MTTF_{avg}$, $\times$ & $E_{avg}$, $\times$ \\
  \hline
  \hline
   2 &   20 & \todo{0} & \todo{0} & \todo{0} \\
   4 &   40 & \todo{0} & \todo{0} & \todo{0} \\
   8 &   80 & \todo{0} & \todo{0} & \todo{0} \\
  16 &  160 & \todo{0} & \todo{0} & \todo{0} \\
  32 &  320 & \todo{0} & \todo{0} & \todo{0} \\
  \hline
\end{itable}
In the first set of experiments, we change the number of cores while keeping the number of tasks per core constant and equal to 10. For each problem we have generated 20 random task graphs of a similar structure and found the average improvement of the MTTF. We also have measured the change in the consumed energy. The results are given in \tabref{tab:mttf-cores}. It can be observed that the temperature-unaware task allocation and scheduling dramatically decrease the lifetime of the device and the optimization based on the SSDTP is a must for an embedded system design framework. Although, the average improvement is decreasing with the growth of the complexity of the problem, it is still considerably high. Note that the energy efficiency of the system is not suffering from the optimization.

\begin{itable}{mttf-tasks}{|r|r|r|r|r|}
  {Reliability optimization for different applications}
  {$N_p$ --- the number of cores, $N_t$ --- the number of tasks, $t_{avg}$ --- the average computational time, $MTTF_{avg}$ --- the average MTTF improvement, $E_{avg}$ --- the average change in the energy consumption.}
  \hline
  $N_p$ & $N_t$ & $t_{avg}, m$ & $MTTF_{avg}$, $\times$ & $E_{avg}$, $\times$ \\
  \hline
  \hline
  4 &  20 & \todo{0} & \todo{0} & \todo{0} \\
  4 &  40 & \todo{0} & \todo{0} & \todo{0} \\
  4 &  80 & \todo{0} & \todo{0} & \todo{0} \\
  4 & 160 & \todo{0} & \todo{0} & \todo{0} \\
  4 & 320 & \todo{0} & \todo{0} & \todo{0} \\
  \hline
\end{itable}
For the second set of experiments, we keep the quad-core architecture and vary the number of tasks within the application. The number of randomly generated task graphs per problem is 20. The average improvement of the MTTF along with the change in the energy consumption are given in \tabref{tab:mttf-tasks}. The observations to be made here are similar to the previous ones: taking into consideration the SSDTP of the system during the design stage can significantly prolong the MTTF without sacrificing the energy efficiency of the system.

\begin{itable}{mttf-comparison}{|r|r|r|r|r|}
  {Reliability optimization for different solution techniques}
  {$N_p$ --- the number of cores, $N_t$ --- the number of tasks, $MTTF^{CE}_{avg}$, $MTTF^{HS}_{avg}$, and $MTTF^{SS}_{avg}$ --- the average improvements of the MTTF obtained by the CE method, TTA with HotSpot, and SS approximation, respectively.}
  \hline
  $N_p$ & $N_t$ & $MTTF^{CE}_{avg}$, $\times$ & $MTTF^{HS}_{avg}$, $\times$ & $MTTF^{SS}_{avg}$, $\times$ \\
  \hline
  \hline
  4 &  20 & \todo{0} & \todo{0} & \todo{0} \\
  4 &  40 & \todo{0} & \todo{0} & \todo{0} \\
  4 &  80 & \todo{0} & \todo{0} & \todo{0} \\
  4 & 160 & \todo{0} & \todo{0} & \todo{0} \\
  4 & 320 & \todo{0} & \todo{0} & \todo{0} \\
  \hline
\end{itable}
Finally, we compare the results of the optimization delivered by the CE method, TTA with HotSpot, and steady-state approximation (SS) discussed in \secref{sec:steady-state-approximation} for the same setup as in the previous set of experiments with the fixed architecture. The final solution found by the later two methods, TTA and SS, are reevaluated using the CE method and compared with the solutions found only by the CE approach. The results are summarized in \tabref{tab:mttf-comparison}.
